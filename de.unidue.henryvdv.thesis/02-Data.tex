\chapter{Data}
\label{sec:Data}

Most machine learning approaches require annotated corpora in order to make the learning process possible. In this case, the training corpora must contain information on the correct antecedent for each anaphora. Since this work is designed to learn gender information through frequencies, a second information source is needed. This chapter will briefly describe both information sources.

\section{WikiCoref }
\label{wikicorefSec}
\cite{wikicoref2016} presented with WikiCoref a coreference-annotated corpus of english Wikipedia articles. Wikipedia differs from most web corpora as it is highly structured. The Wikipedia guidelines\footnote{https://en.wikipedia.org/wiki/Wikipedia:Manual\_of\_Style} contain various restrictions on grammar and vocabulary and also define the structure of articles in terms of sections and paragraphs. In contrast, most other web-mined sources are heavily unstructured and could contain colloquial language as well as ungrammatical texts. \\
An excerpt of 30 articles was used to build the corpus. \citep{wikicoref2016} figured out that more than 35 \% of all Wikipedia articles contain less than 100 words and only 11 \% more than 1000 words. Articles with few word counts (less than 200 words) were not considered as they do not contain enough information for meaningful coreference resolution. Hence articles cannot be chosen completely random, a uniform distribution of categorized article sizes leading from less than 1000 to more than 5000 words was strived. Additionally, articles with too many out links were not considered. In order to keep the corpus domain-independent, articles on different topics were selected.\\

To detect entities, a combination of a coreference resolution system, an entity detection module, and anchored links in the article was used. The coreference chains detected by the module were manually corrected and missing ones were added. All coreferring entities were linked through a joint identification number representing the real-world entity.

\begin{addmargin}[0.5cm]{0.5cm}
[Aberfoyle]\textsubscript{1} is [a village in the region of Stirling, Scotland, northwest of [Glasgow]\textsubscript{2}]\textsubscript{1}.

[The town]\textsubscript{1} is situated on [the River Forth]\textsubscript{3} at the base of [Craigmore]\textsubscript{4} (420 metres high).
\end{addmargin}

In total, the corpus contains 59652 tokens\footnote{``A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing." (http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)} in 2229 sentences with an average of 2000 tokens per document. For the inter-annotator agreement an MUC F1 score \citep{vilain1995model} of 83,3 \% was reported.

\section{Gender Corpus}
\label{section:gendercorpus}
An automatic approach to learning gender information through corpus- and web-based frequencies was introduced by \cite{bergsma2005automatic} and explained in Section \ref{section:bergsma2005automatic} of this work. 
\cite{Bergsma:06} pointed out two disadvantages of the previous approach. First of all, sending Google requests for each possible antecedent on large corpora is time-consuming and therefore not cost-efficient. Secondly, the corpus and web based implementations are not symmetric as some occurrences can only be found with the web-based approach. For instance, the corpus-based approach merely accepts a verb between a noun and a reflexive pronoun while the Google wildcard operator (``*") is not limited to any grammatical category.
Therefore, a new corpus mined frequency distribution of gender and number information was mined using a corpus of approximately 85 GB. Overall, an accuracy of 90,3 \% on gender determination was reported. \cite{Bergsma:06} made the mined gender and number frequencies openly accessible for the NLP community.\footnote{Available for download at http://www.clsp.jhu.edu/\textasciitilde sbergsma/Gender/Data/}