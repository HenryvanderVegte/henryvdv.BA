\chapter{Introduction}
\label{sec:Introduction}

\section{Background}

In the last decades, the amount of textual information in media has increased severely, making automatic text comprehension indispensable. Since textual data found online is mostly unstructured, which means that there is no formal structure in pre-defined manner, various information needs to be added in order to make automatic understanding possible. For several natural language processing (NLP) tasks referential relationships between words  in a document need to be set. 

The procedure of determining whether two expressions  refer to each other, meaning that they are instances of the same entity, is called anaphora resolution. The word to be resolved is termed anaphora while its predecessor is the antecedent. It differs from coreference resolution by considering only those words which get their meaning through its antecedent \citep{recasens2007anaphora} (1), while all corefering expressions are considered in coreference resolution (2).

\begin{addmargin}[1cm]{1cm}
(1) [Aberfoyle] describes [itself] as [The Gateway to [the Trossachs]]. (resolve “itself" to “Aberfoyle")

(2) As late as 1790, all the residents in the parish of [Aberfoyle] spoke [Scottish Gaelic]. From 1882 [the village] was served by [Aberfoyle railway station]. (resolve “the village"  to “Aberfoyle")
\end{addmargin}

Resolving noun phrases is a growing task in Natural Language Processing (NLP) and increased its relevance in the last decades, that it even became a standalone subtask in the DARPA Message Understanding Conference in 1995 \citep{chinchor1995message}. The International Workshop on Semantic Evaluation (SemEval) conducted a coreference resolution task on multiple languages \citep{recasens2010semeval} emphasizing its importance. 
There are several fundamental applications of coreference and anaphora resolution, such as Information Extraction (IE) \citep{mccarthy1995using} and Question Answering (QA) \citep{morton2000coreference}.\\ 
Information Extraction targets to summarize relevant information from documents. Anaphora resolution is required as the quested entity is often referenced through various words, amongst others personal pronouns. \citep{mccarthy1995using} described the latter as a classification problem: “Given two references, do they refer to the same object or different objects."\\
The question answering task described by Morton seeks to find a 250 byte string excerpt out of a number of documents as the answer for a query. Annotated coreference chains were used to link all instances of the same entity in a document. Occurrences in another sentence are given a lower weight for prediction. The use of annotated coreference chains improved the prediction slightly.\\
Various information sources including syntactic, semantic, and pragmatic knowledge are needed since selecting a possible antecedent is a decision under high ambiguity. The decisive factor for determination might be e.g. gender agreement or the distance between antecedent and anaphora. Sometimes there is no decisive factor at all. Examples for the importance of gender agreement are shown in (3) and (4), the influence of word distance could be simplified described as it is more likely to find the antecedent in proximity to its anaphora.

(3) John and Jill had a date, but he didn't come. (resolve “he" to “John").

(4) John and Jill had a date, but she didn't come. (resolve “she" to “Jill").

%-> PRONOUN RESOLUTION: A.I. complete' task-- (CogNIAC)
%Cataphora 
%Pleonastic it

\section{Motivation}
Significant factors of uncertainty are gender and number, because they are hard to determine. At first, information is needed whether a noun is male, female, neutral, or plural. Honorifics like “Mr." and “Mrs." are gender indicators, but not sufficient due to their sparsity. Stereotypical occupations and gender indicating suffixes like policeman and policewoman turned out to be no longer reliable \citep{evans2000improving}. For that reason, gender and number information needs to be learned from an external source. 

There are two different strategies for implementing reliable gender information: \\
Firstly, gender can be treated as a hard constraint. This means that either the most likely gender is assigned or, in case of uncertainty, no assignment is made at all. The leading coreference resolution systems mostly use hard constraint gender information \citep{soon2001machine}. The gender of to the most frequent sense of a noun is assumed.\\
Secondly, gender can be expressed through probabilities. If a noun is male in 70 of 100 cases, the probability for it to be male is 70 \% (note that this is simplified - the distribution will be smoothed to avoid 0-probabilities). In \nocite{bergsma2005automatic} 2005, Bergsma obtained encouraging results with the use of gender probabilities. More precisely, adding corpus mined gender frequencies improved their accuracy by approximately 10 \%.

This work will present a machine learning approach to anaphora resolution, focusing on third-person pronominal anaphoras. The two main purposes are to determine the impact of gender probability and to compare it to gender information treated as a hard constraint. First of all, it should be evaluated whether the improvement through gender frequencies can be replicated on different data sets. In a second step, the gender frequencies will be replaced by the assignment of the most frequent gender to examine the influence of nothing but the gender implementation strategy. This is necessary as usage of different data sets and algorithms makes the comparison of papers inconclusive. Finally, it needs to be examined whether the hypothesis that corpus based gender frequencies have a higher impact than gender constraints can be confirmed.




