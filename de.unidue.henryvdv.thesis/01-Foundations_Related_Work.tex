\chapter{Related Work}
\label{sec:Related Work}

Anaphora resolution systems emerged into two different strategies. First of all, there are rule-based techniques which focus more on theoretical considerations. The second strategy uses machine learning and is based on annotated data. The following chapter will briefly present both and discuss their advantages and disadvantages, followed by exemplary implementations and approaches. Especially the handling of gender information is of interest. Since anaphora resolution is a subtask of coreference resolution, coreference resolution systems will be considered as well.

\section{Rule-Based Approaches}
Rule-based techniques rely on manual understanding and implementation of syntactic and semantic principles in natural language \citep{kennedy1996anaphora,mitkov1994integrated,ingria1989computational}. Clues that could be helpful for antecedent identification are manually implemented as rules. To identify relevant clues, prior knowledge about linguistic principles (such as binding principles) is necessary. Since rules might be domain-specific, the implementation would most likely be worse on other domains. Refinements for different domains would make the development even more complex and time-consuming. Nevertheless, rule-based techniques are much more transparent in contrast to machine learning. In the last section, a comparing evaluation of both techniques will be presented.

\subsection{The Naive Hobbs algorithm}
The Naive Hobbs algorithm described by \cite{hobbs1978resolving} relies on parsed syntax trees containing the grammatical structure. Put simply, the tree containing the anaphora is searched left-to-right with breadth-first search and the algorithm stops when a matching noun phrase is found. Noun phrases mismatching in gender or number are neglected. Information on gender could only be selected through surface clues. The algorithm also limits the list of possible antecedents, as for instance the antecedent cannot occur in the same non-dividable noun phrase. As long as no matching antecedent is found, the preceding sentence will be searched successively. \\
Hobbs reported an accuracy score of 88.3 \% on the pronouns ``he",``she",``it", and â€œthem" with only using the algorithmic approach. A greater insight into parsed syntax trees will be given in Section \ref{preprocessingSection}.

\subsection{High Precision Pronoun Resolution}
\label{baldwinCogNIAC}
Another rule-based approach was presented by \citep{baldwin1997cogniac} with CogNIAC, a high precision pronoun resolution system. It only resolves pronouns when high confidence rules (shown in Table \ref{table:cogniacRules}) are satisfied in order to avoid decisions under ambiguity and to ensure, that only very likely antecedents are attached (high precision). This might lead to a neglect of less probable but still correct antecedents and could therefore lower the recall score. 
\\
For each pronoun, the rules are applied one by one. If the given rule has found a matching candidate, it will be accepted. Otherwise, the next rule will be applied. If none matches the candidates, it will be left unresolved as this implicates a higher ambiguity. In order to apply Baldwin's high confidence rules, information on sentences, part-of-speech, and noun phrases is required and therefore annotated. Gender and number information is searched in various databases and assigned if a match is found. The algorithm excludes possible candidates that mismatch in gender or number. \\
As can be seen, the order of rules leads from higher to lower precision: if only one possible antecedent can be found (rule 1) it is most likely the correct antecedent, while rule 6 indicates more ambiguity as it relies on more content-related information. Human understanding of syntax and semantics is needed to determine a specific order of rules. Therefore, adding new rules might not improve the performance even though those rules are reasonable in itself. Most rule-based systems struggle with that problem.\\
Confirming their prediction, \cite{baldwin1997cogniac} reported a high precision (97 \%), but inferior recall score (60 \%) on their training data consisting of 198 pronouns.
In a second evaluation, CogNIAC was compared to the Hobbs Algorithm \citep{baldwin1997cogniac,hobbs1978resolving} on singular third-person pronoun resolution. In order to maximize the ambiguity, the training data texts were narrations about same gender characters. To make accuracy scores comparable, \cite{baldwin1997cogniac} added lower precision rules, such as the most recent antecedent should be picked if no other rule found a matching noun phrase. The Accuracy scores reported were nearly equal (78.8\% on the Hobbs Algorithm, 77.9\% on CogNIAC), underlining the reason of existence of various approaches.

\begin{table}[h]
  \caption{CogNIAC core rules}
    \begin{tabular}{| l |p{8cm} |}
    \hline
    Rule & Description \\ \hline
\hline
    1) Unique in Discourse & If there is a single possible antecedent PAi in the read-in portion of the entire discourse, then pick PAi as the antecedent. \\ \hline
    2) Reflexive & Pick nearest possible antecedent in read-in portion of current sentence if the anaphora is a reflexive pronoun \\ \hline
    3) Unique in Current + Prior & If there is a single possible antecedent i in the prior sentence and the read-in portion of the current sentence, then pick i as the antecedent: \\ \hline
    4) Possessive Pro & If the anaphora is a possessive pronoun and there is a single exact string match i of the possessive in the prior sentence, then pick i as the antecedent:  \\ \hline
    5) Unique Current Sentence & If there is a single possible antecedent in the read-in portion of the current sentence, then pick i as the antecedent  \\ \hline
    6) Unique Subject/ Subject Pronoun & If the subject of the prior sentence contains a single possible antecedent i, and the anaphora is the subject of the current sentence, then pick i as the antecedent \\ \hline
    \end{tabular}
     \label{table:cogniacRules}
\end{table}

~\\
\subsection{Anaphora Resolution with Limited Knowledge}
\label{anaphoraLimitedKnowledgeSection}
A domain independent approach by \cite{mitkov1998robust} tried to eliminate the disadvantages of previous rule-based systems. Mitkov renounced complex syntax and semantic analysis in order to keep the algorithm as less domain specific as possible. Only a part-of-speech tagger and a simple noun phrase identifitcation module were applied. The algorithm was informally described by Mitkov in three steps:
\begin{enumerate} 
\item Examine the current sentence and the two preceding sentences (if available). Look for noun phrases only to the left of the anaphora
\item Select from the noun phrases identified only those which agree in gender and number with the pronominal anaphora and group them as a set of potential candidates
\item Apply the antecedent indicators to each potential candidate and assign scores; the candidate with the highest aggregate score is proposed as antecedent
\end{enumerate}
Similar to the previously described approaches, gender and number information was derived through lists. Nouns that do not necessarily agree in gender with its pronoun (``team" for instance can be singular or plural) were not filtered.
Overall, a set of 10 antecedent indicators were used which indicate either a high or a low likelihood for the noun phrase to be the antecedent. Negative indicators such as definiteness (whether the noun phrase contains a definite article, whereby indefinite phrases decrease the likelihood) and positive indicators like term preference (if the noun phrase is a term in the field, the likelihood is increased). The score values are integers from -1 to 2. \\
Mitkov reported a success rate of 89.7 \% on random sample texts of technical manuals. A modified approach could also be applied for polish \citep{mitkov2000robust} and arabic \citep{mitkov1998multilingual} with similar success rates.
A comparing evaluation to Baldwins CogNIAC \citep{baldwin1997cogniac} indicated a superiority of Mitkovs approach \citep{mitkov1998robust} as CogNIAC had a lower success rate of approximately 15 \% on the previously described data set. The stated reason for the comparison was that the approaches showed several similarities as both require few preprocessing and gain their information mostly from part-of-speech tags and noun phrases.\\
The superiority of \citep{mitkov1998robust} could be explained by its handling of uncertainty as the antecedent indicators are not implemented as hard constraints. Basically, Mitkovs anaphora resolution system can be described as a combination between rule-based and statistical techniques in order to use the best of both worlds.
In 2002, a revised version of the original approach by Mitkov was presented \citep{mitkov2002new}. An improved version called MARS implemented a few changes:\\
First of all, three new antecedent indicators and a module for identification of pleonastic pronouns\footnote{A pleonastic pronoun is non-referential. For example, the \textit{it} in ``it is raining" } and non-nominal pronominal anaphoras were added. Additionally, the implementation of some previous features was changed as other preprocessing tools were used.

A modular anaphora resolution tool called GuiTAR relying on Mitkovs MARS-algorithm \citep{mitkov2002new} was developed by \cite{poesio2004general}. It was designed to be domain-unspecific and usable off-the-shelf which means that preprocessing steps such as part-of-speech tagging and named entity recognition will be added on itself. Either raw text data or XML files can be used as the input. In case of raw text data, XML files with annotated part-of-speech tags, noun phrase boundaries, pronoun categories etc. will be created. \citep{poesio2004mate} reported an F-measure of 64.2 \% for personal pronouns on raw text data of the GNOME corpus \citep{poesio2004general}. In comparison, the baseline approach (choosing the most recent antecedent) achieved an F-measure of 50.5 \% on the same data.

\subsection{Discourse Knowledge-Based Anaphora Resolution}
\label{aonediscourseMD}
A manually designed anaphora resolution system by \citep{aone1993language} was built to be language-independent, extensible, robust, and tunable for specific domains. The used information was derived through three different knowledge bases: the \textit{Discourse Knowledge Source}, the \textit{Discourse Phenomenon}, and the \textit{Discourse Domain}. \\
The former contains antecedent generators to determine all possible antecedents, a system to filter out unwanted antecedent candidates, and an orderer to rank the candidates from highest to lowest likelihood. All of these components rely on specific rules and functions. For instance, the filter removes candidates of mismatching gender. Even though some the rules are only applied on specific languages, \cite{aone1993language} reported that most of them are language-independent.\\
The \textit{Discourse Phenomenon} contains all possible part-of-speech categories in which the anaphora could occur in a hierarchical order. For instance, ``third-person pronoun" is a subclass of ``pronoun". Each class includes its definition, two resolution strategies (a second one is needed if the main strategy fails), and specific language information if a category only exists in a certain language.\\
The third knowledge base is responsible for domain-specific information. 

A module called \textit{Discourse Administrator} was used to determine the application domain and in a further step to select and filter the knowledge bases in order to generate the best possible resolution system. Therefore, the information stored in each knowledge base is heavily dependent on the considered language and domain. The general resolution process is as follows: The discourse phenomena are used to determine all anaphoras. In a second step, the discourse knowledge sources are applied in order to generate and filter all possible candidates (candidates with mismatching gender were neglected). A syntactic gender filter was applied. If only one candidate remains, it will be chosen as antecedent. Otherwise, one or more orderers are applied and the best candidate will be chosen by order. If no candidate was found at all, the second strategy specified in the discourse phenomenon will be applied. A comparing evaluation to another machine learning based approach by \cite{aone1995evaluating} will be presented in Section \ref{aoneBennetEval}.

\section{Machine Learning-Based Approaches}

Most machine learning-based techniques learn principles from annotated text corpora \citep{soon2001machine, bergsma2005automatic}, which include the correct label for each instance. In this context, a label will contain the information whether a noun phrase is the antecedent. A decisive factor of machine learning is that irrelevant information (presented through features) has a lower impact on success factors (the accuracy for instance) compared to rule-based techniques, as the algorithm automatically learns to rate those as irrelevant and vice versa. Machine learning approaches tend to have little information on linguistic principles as the algorithm should learn principles autonomously. This causes the algorithm to be less domain specific, but increases the risk to miss relevant clues. However, top-performing machine learning approaches achieve accuracy scores comparable to best non-learning techniques \citep{soon2001machine}. \\
Additionally, machine learning algorithms are usually more time-consuming due to its learning process.

\subsection{Machine Learning Based Coreference Resolution}
\label{soon2001traininginstances}

As already stated, coreference resolution aims for linking all noun phrases referring to the same entity in the real world in a document. The most common kind of storing coreferential information is through coreference chains, in which the current element always points towards the following same entity-element. Another way of storing coreferences is to define a unique ID for each real-life entity. All occurrences in the text will be assigned to their belonging IDs.

An often quoted coreference resolution system using machine learning was proposed by \cite{soon2001machine}. In this case, a decision tree classifier was chosen. A natural language processing pipeline was used for the identification of markables. The pipeline identified amongst other annotations part-of-speech tags, noun phrases, named entities, and semantic classes. A high value was placed on designing generic features to make the classifier domain-independent. In total, a set of 12 different features was used. It covers inter alia a distance feature (standing for the distance in sentences between two elements), a gender agreement feature (whether the gender matches), and a number agreement feature (whether the number matches). Deriving gender information of a noun requires information of their semantic classes. Semantic classes are for instance ``person", ``organization", or ``male". \cite{soon2001machine} worked with the simplified assumption that the semantic class of a noun phrase is the semantic class of the most frequent sense of the considered noun in WordNet. Gender agreement was assumed if both phrases got the same semantic class (for example ``male") or if one is the parent of the other (for instance, if one phrase is considered as ``person" and the other as ``male"). 
In order to make machine learning possible, training instances need to be generated.\\
To generate positive training instances, \cite{soon2001machine} used every noun phrase in a coreference chain and its most recent mention in the document. Each intervening noun phrase forms a negative instance with the considered noun phrase. 

The researchers reported an F-measure of 62.6 \% on the MUC-6 data and comparable results on the MUC-7 data. A comparison with official MUC-7 systems indicated that their system performed at the upper bound of all considered systems. The coreference resolution system implemented by \cite{soon2001machine} and its feature set were often referred as a baseline for further systems \citep{versley2008bart}.

\cite{ng2002improving} extended their work and improved it through additional features, a different training set creation, and a clustering algorithm to find the noun phrase with the highest likelihood of coreference. The majority of the new features is based on syntactical principles. For instance, binding constraints must be fulfilled (a further explanation on binding constraints is given in Section \ref{preprocessingSection}) and one phrase is not allowed to span another. Positive training instances are not created through the most recent antecedent, but through the most confident one. In addition, they started to search for a related antecedent from right-to-left for a highly likely antecedent (in contrast to starting the right-to-left search for the first previous noun phrase). \cite{ng2002improving} reported a significant increase in precision and F-measure compared to the initial approach by \cite{soon2001machine}.

In 2008, Versley et al. introduced a coreference resolution system for raw text data which extended the previously described approach by \cite{soon2001machine} called BART. The ambition for BART was to keep it as modular as possible so that it could be applied to many different subtasks of coreference resolution. BART consists of a preprocessing pipeline for parsing, part-of-speech tagging, and further basic information and a mention factory for mainly gender and number identification. Additionally, a feature extraction module and therefore a matching decoder and encoder is included. The decoder generates the training data while the encoder prepares the testing data. Similar to \citep{soon2001machine} the feature labels are binarized which means that an anaphora either contains the correct or wrong antecedent. Accordingly, the feature labels are either true or false. \\
A subsequent approach on multiple languages with BART \citep{broscheit2010bart} used a set of seven features for all classification types, including a gender agreement, number agreement, string match, and distance feature. The procedure of gaining gender and number information was adopted by \cite{soon2001machine}. 

An F-measure of approximately 55.6 \% on Bnews articles of the ACE-2 corpora was reported with the usage of the basic feature set \citep{versley2008bart}. 
With additional language-dependent features, BART was also transferred to German \citep{broscheit2010extending}, Polish \citep{kopec2012creating}, and Italian \citep{poesio2010creating}.

However, \cite{reiteretal:2011b} indicated that a great weakness of BART is the implementation of gender information as in their evaluation even noun phrases with explicit gender information were linked incorrectly.

\subsection{Pronoun Resolution in Spoken Dialogue}
As already mentioned, machine learning approaches are less domain-specific than rule-based systems. For that reason \cite{strube2003machine} presented a corpus-based approach for pronoun resolution in spoken language. Still, several extensions and adaptions had to be done as spoken dialogue differs from written texts significantly. First of all, the number of pleonastic pronouns in spoken dialogue is substantially increased. Secondly, a not ignorable amount of anaphoras in spoken dialogue do not have a clearly defined antecedent so that even humans cannot determine them. \cite{eckert2000dialogue} called them vague anaphoras and figured out that 13.2 \% of all anaphoras in their examined corpus fall into that category.\\
A corpus of twenty switchboard dialogues was used by \cite{strube2003machine}. In order to generate training data, a list of all potential anaphoras was created. Potential anaphoras are all non-definite noun phrases except for first and second person pronouns. Each element in the remaining list forms a pair with every preceding noun phrase that does not disagree in gender, number, or person. If the instances corefer they were labelled P, else N. For all anaphoras without explicit noun phrase antecedents, other phrases (for instance verb phrases) in the last two sentences were used to form pairs. \\
The feature set with a total of 25 features included noun phrase features, coreference level features, and spoken dialogue features. Noun-phrase features such as gender, number, or the grammatical function of the anaphora or the antecedent rely on preprocessing. Coreference level features could be described as low-level preprocessing features. Those features mainly describe the distance between the antecedent and the anaphora, for instance, in words or sentences. The features especially for spoken dialogue contain, for instance, information on how many noun phrases are located between anaphora and antecedent.
A decision tree classifier with 20-fold cross-validation was applied. \citep{strube2003machine} reported an F-measure of 47.42 \% for the full classifier, including all pronouns and all features. 

\subsection{Corpus and Web Mined Gender Information}
\label{section:bergsma2005automatic}
\cite{bergsma2005automatic} presented a machine learning approach to anaphora resolution which treats gender information not as a hard constraint, but as a probability distribution of possible outcomes. A majority of previous approaches assigned either a specific gender and number (e.g. masculine, feminine, neutral, or plural) or, in case of uncertainty, no gender at all \citep{soon2001machine, broscheit2010bart}. Another motivation was that \cite{kennedy1996anaphora} reported to attribute 35 \% of their resolution errors to gender mismatch. Only third-person pronouns were considered.

The gender information was derived from two sources: a text corpus and the web. 
For the former, all occurrences of nouns and pronouns in lexico-syntactic patterns are counted. Five different patterns for reflexives, possessives, nominatives, predicates, and designators were used (Table \ref{table:bergsma2004GenderTable}). A reflexive masculine occurrence would be for instance ``John likes himself". In this case, a counter for \textit{John} with masculine gender and reflexive pronoun will be increased. This procedure was repeated for all other patterns and remaining genders and numbers. \cite{bergsma2005automatic} applied lots of textual data in order to offset parser errors and other noise sources. The whole data set included the AQUAINT corpus \citep{graff2002aquaint} as well as the Reuters corpus \citep{rose2002reuters}. In total, a data set of approximately six gigabytes of text was used.

\begin{table}[h]
  \caption{Gender corpus patterns}
    \begin{tabular}{| l | p{5cm} | p{5cm} |}
    \hline
    Gender Corpus Indicators & Contained Elements & Pattern \\ \hline
\hline
    1) Reflexive & himself, herself, itself, and themselves &  \textit{noun} + \textit{verb} + \textit{reflexive}\\ \hline
    2) Possessive & his, her, its, and their & \textit{noun} + \textit{verb} + \textit{possessive} + \textit{noun} \\ \hline
    3) Nominative & he, she, it, and they & \textit{noun} + \textit{verb} + \textit{nominative} +  \textit{verb} \\ \hline
    4) Predicate & he, she, it, and they & \textit{pronoun} + is/are [a] + \textit{noun}  \\ \hline
    5) Designator & Mr. and Mrs. & \textit{designator} + \textit{noun}\\ \hline
    \end{tabular}

     \label{table:bergsma2004GenderTable}
\end{table}

Since a text corpus, no matter how big it is, cannot contain all possible words and word combinations, the web was used as a second information source. The Google API was used to count the web pages that appear if a noun in combination with the Google wildcard operator (``*") and the gender indicator (e.g. the pronoun) was requested. For instance, if the gender of \textit{John} should be determined, Google requests will be sent with all gender indicating elements of Table \ref{table:bergsma2004GenderTable} ( \textit{John * himself}, \textit{John * herself}, \textit{John * itself}, etc.).
In the following step, the probabilities for each gender were determined through the five corpus sources and the five web sources. The naive approach would be that the probability of the indicator to be masculine is the percentage of all cases in that the word occurs with its masculine indicator. For instance, in Table \ref{table:bergsma2004GenderFreqTable} the cumulated frequency of \textit{Alex} occurring with \textit{himself} is 60. In total, \textit{Alex} was found 100 times with a reflexive pronoun. As a consequence, the probability for \textit{Alex} to be masculine would be estimated at 60 \% from reflexive indicators.
This approach leads to three major problems. First of all, zero-probabilities would indicate that there is no possibility for noun to belong to that gender. This might be true - some words might never be part of a certain gender. However, it might just be a rare event and an occurrence would be found with a larger or different text corpus. Secondly, adding a further count could change the likelihood enormous for small frequencies. This leads to the third problem: a measure is needed to determine the certainty of a likelihood. A 70 \% probability of a word to be masculine is more meaningful if 1000 cases are considered rather than 10. 
 
In order to solve those problems, \cite{bergsma2005automatic} treated the counts as \textit{Beta} distributions in a Bayesian approach. More precisely, two parameters named $\alpha$ and $\beta$ were considered. For each gender, $\alpha$ determines the count of the considered event plus one (in order to avoid zero-probabilities) while $\beta$ represents the count of all not considered events plus one. The $\alpha$ and $\beta$ values of the previous \textit{Alex} example with reflexive indicators for masculine gender would be $\alpha$ = 61 and $\beta$ = 41. The mean value of it is computed as:
\begin{center}
	 $\mu$ =  $\dfrac{\alpha }{\alpha + \beta}$  
\end{center}

A complete distribution is presented in Table \ref{table:bergsma2004GenderFreqTable}. It should be noted that, unlike the naive approach, these values can only be partially compared to one another as each of value represents a single distribution. Furthermore, the percentages do not sum up to 100 \%.

\begin{table}[h]
      \caption{Gender frequencies example}
	\centering
    \renewcommand{\arraystretch}{2.0}
    \begin{tabular}{| l | l | l | l |}
    \hline
    Gender/Number & Occurrences & Naive Approach & Bayesian Approach \\ \hline
\hline
    1) Masculine & 60 & $\dfrac{60}{100}$ = 60 \% &  $\dfrac{61}{102}$ = 59.8 \% \\ \hline
    2) Feminine & 30 &  $\dfrac{30}{100}$ = 30 \% &  $\dfrac{31}{102}$ = 30.4 \% \\ \hline
    3) Neutral & 10 & $\dfrac{10}{100}$ = 10 \% & $\dfrac{11}{102}$ = 10.8 \%  \\ \hline
    4) Plural & 0 &    $\dfrac{0}{100}$ =  0 \% &  $\dfrac{1}{102}$ = 0.1 \% \\ \hline
    \end{tabular}

     \label{table:bergsma2004GenderFreqTable}
\end{table}
~\\

\cite{bergsma2005automatic} expressed the certainty through the variance of Beta distributions:

\begin{center}
	$\sigma$\textsuperscript{2} =  $\dfrac{\alpha \beta }{(\alpha + \beta)\textsuperscript{2}(\alpha + \beta + 1)}$
\end{center}

In case of little or no counts at all, the variance will be approximately 1/12. The classifier should automatically learn that distributions with that variance will not be certain.

In order to prove the accuracy of their gender classification, \cite{bergsma2005automatic} built several SVM-Classifiers. Overall, a set of 20 features was used: Each of the five gender indicators (reflexive, possessive, etc.) has its mean and its variance as features (in this case, the standard deviation was used which is the square root of the variance). Each of the gender indicators was implemented corpus-based as well as web-based. All gender features led to an F-measure of 92 \% on gender determination. Separate classifiers for either web-based or corpus-based information yielded to an F-measure of 85.4 \% for the corpus-based and 90.4 \% for the web-based approach.

Various pronoun resolution classifiers were built in order to determine the influence of several aspects. In general, each classifier searches, beginning by the certain anaphora, the text backwards until a matching antecedent is found. The search backwards was limited so that only the current and the previous sentence were considered, because a corpus observation showed that more than 97 \% of all antecedents could be found in that range. If no accepted antecedent was found the threshold for acceptance was reduced so that antecedents with lower likelihood might be accepted. Then the search started again at the first preceding noun phrase. This procedure was repeated until the first candidate exceeds the threshold. 

The baseline approach selected always the most recent noun phrase as antecedent. An accuracy of 26.0 \% was reported. 

A first improvement consisted of the sole use of explicit gender indicators such as ``Mr." and ``Mrs." to determine the gender. The first previous antecedent that does not mismatch in gender was chosen. The accuracy was improved up to 30.8\%. 
In a third baseline approach, the previously mentioned gender SVM-classifiers were used to detect a gender match or mismatch. Underlining the importance of gender and number agreement, the accuracy rose up to 59.4 \%.

The first machine learning approach included a feature set of 42 features, whereby most of the features were binarized. The features can be separated into four categories. First of all, there are pronoun-related features that determine the gender and number of the pronoun. Secondly, antecedent-related features provided for instance information on the grammatical relation of the noun phrase or whether it is a person or an organization. The third group of features describes the relation of the pronoun and its antecedent and contains features that rely on linguistic principles (such as if binding principles are satisfied) as well as features that only require basic preprocessing steps (sentence and word distance, for instance). The fourth category affects the standard gender implementation, including features for gender match, gender mismatch, and pronoun mismatch (no corpus mined gender frequencies were used). In order to apply those features the texts were tokenized, parsed, and noun phrases were linked. The training instance creation procedure was adopted by \cite{soon2001machine} and was previously described in Section \ref{soon2001traininginstances}. In total, 1251 positive and 2909 negative training instances were created. 

The classifier reached a performance score of 62.3 \% which is above all baseline approaches. The additional use of corpus and web frequency features led to a performance score of 73.3 \%. 

\subsection{Anaphora Resolution with Various Parameters}
\label{aoneBennetEval}
A machine learning-based anaphora resolution system presented by \cite{aone1995evaluating} used pairwise training examples containing information on the anaphora and its possible antecedent. A whole set of 66 features was used. \cite{aone1995evaluating} divided most of them into one of four subcategories namely lexical, syntactic, semantic, and positional. The feature set was inspired by the manually designed resolver of \cite{aone1993language} (described in Section \ref{aonediscourseMD}), but was generalized and changed in order to be domain and language independent.

In total, six different classifiers depending on three parameters were trained. The first parameter was called anaphoric chain. If its value is true, a correct antecedent is detected if the candidate is part of the same anaphoric chains which means that both refer to the same real-world entity. Otherwise, only the preceding same-world entity will be accepted as correct antecedent. This parameter also affects the training instance generation. In case of anaphoric chains, all corefering phrases will form positive training instances with its anaphora. In the other case, just the preceding corefering phrase will be used for positive instances. In both cases, the remaining phrases will form negative training instances with the anaphora. The second parameter determines whether the decision tree will use further information of the anaphoric type (for instance whether the real-world entity of the anaphora is a proper name). A third parameter determines the pruning-factor of its decision tree. A high pruning-factor indicates a higher generalization while decision trees with a lower factor tend to overfit. 

A comparison with the manually designed resolver (MDR) by \cite{aone1993language} was evaluated on Japanese newspaper articles. Since Japanese contains so-called zero-pronoun which are pronouns that are not explicitly mentioned in the text, the results can only partially be compared to English. However, the comparison of the approaches among themselves could still be informative. In total, 1271 anaphoras were used. As it can be seen in Table \ref{table:aone1995evals}, all machine learning approaches using anaphoric chains outperformed the manual approach independent of their pruning-factor, while the approach without the usage of anaphoric chains performed slightly worse. The different pruning factors seemed to have a rather low impact on the performance.

As the manually designed resolver also detects only the preceding same-world entity, it would be most reasonable to compare it to the MLR-6. Even though the manual approach performed better, no language specific information or relevance of features needed to be determined on the machine learning-based resolver as the algorithm learned it autonomously \citep{aone1995evaluating}. \cite{aone1995evaluating} interpreted the results as optimistic for machine learning techniques.

%Zero-pronouns!

\begin{table}[h]
  \caption{Parameter dependent evaluation and comparison}
\begin{tabular}{|c|c|c|c|c|}
	\cline{1-5}
	Algorithm & Anaphoric Chains & Anaphoric Type& Confidence & F-measure \\ \cline{1-5}
	\cline{1-5}
	MLR-1 & yes & no & 100 \% &  76.27 \\ \cline{1-5}
	MLR-2 & yes & no & 75 \% & 77.30 \\ \cline{1-5}
	MLR-3 & yes & no & 50 \% & 76.43 \\ \cline{1-5}
	MLR-4 & yes & no & 25 \% & 77.28 \\ \cline{1-5}
	MLR-5 & yes & yes & 75 \% & 74.54 \\ \cline{1-5}
	MLR-6 & no & no & 75 \% & 67.03 \\ \cline{1-5}
	\multicolumn{5}{r}{}\\ \cline{4-5}
	\multicolumn{2}{r}{} & &  MDR & 69.57 \\
\cline{4-5}
	\end{tabular}

     \label{table:aone1995evals}
\end{table}


